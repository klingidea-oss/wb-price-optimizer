"""
WB Price Optimizer V3.0 - ML Grouping Engine
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ —Ç–æ–≤–∞—Ä–æ–≤
"""

import json
import re
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from collections import defaultdict
from typing import List, Dict, Tuple
import pickle


class MLGroupingEngine:
    """
    –î–≤–∏–∂–æ–∫ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ —Ç–æ–≤–∞—Ä–æ–≤-–∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤
    """
    
    def __init__(self):
        self.vectorizer = TfidfVectorizer(
            max_features=500,
            ngram_range=(1, 3),
            analyzer='char_wb',
            lowercase=True
        )
        self.category_patterns = {}
        self.trained = False
        self.similarity_threshold = 0.75  # –ü–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏ 75%
        
    def extract_features(self, product: Dict) -> str:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ —Ç–æ–≤–∞—Ä–∞ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
        """
        # –û—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–ª—è
        name = product.get('name', '').lower()
        category = product.get('category', '').lower()
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è
        size = self._extract_size(name)
        material = self._extract_material(name)
        color = self._extract_color(name)
        type_info = self._extract_type(name)
        
        # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º –≤—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        features = f"{category} {type_info} {material} {size} {color} {name}"
        
        return features.strip()
    
    def _extract_size(self, text: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã: 150—Ö250, 150x250, 150*250, 150 —Ö 250 —Å–º
        patterns = [
            r'(\d{2,4})\s*[x—Ö*√ó]\s*(\d{2,4})',  # 150—Ö250
            r'(\d{2,4})\s*—Å–º\s*[x—Ö*√ó]\s*(\d{2,4})\s*—Å–º',  # 150 —Å–º —Ö 250 —Å–º
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text.lower())
            if match:
                return f"{match.group(1)}x{match.group(2)}"
        
        return ""
    
    def _extract_material(self, text: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–∞—Ç–µ—Ä–∏–∞–ª–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        materials = [
            '–±–ª—ç–∫–∞—É—Ç', 'blackout', '–±–ª–µ–∫–∞—É—Ç',
            '–∫–∞–Ω–≤–∞—Å', 'canvas',
            '–±–∞—Ä—Ö–∞—Ç', '–≤–µ–ª—é—Ä',
            '–ª–µ–Ω', '–ª—å–Ω—è–Ω–æ–π',
            '—Ö–ª–æ–ø–æ–∫', 'cotton',
            '–ø–æ–ª–∏—ç—Å—Ç–µ—Ä', 'polyester',
            '—à–µ–ª–∫', 'silk',
            '—Ç—é–ª—å', '–æ—Ä–≥–∞–Ω–∑–∞', '–≤—É–∞–ª—å',
            '–∂–∞–∫–∫–∞—Ä–¥',
            '–æ–¥–Ω–æ—Ç–æ–Ω', '–æ–¥–Ω–æ—Ç–æ–Ω–Ω—ã–π',
            '–∞–ª—é–º–∏–Ω–∏–π', '–∞–ª—é–º–∏–Ω–∏–µ–≤—ã–π',
            '–ø–ª–∞—Å—Ç–∏–∫', '–ø–ª–∞—Å—Ç–∏–∫–æ–≤—ã–π',
            '–º–µ—Ç–∞–ª–ª', '–º–µ—Ç–∞–ª–ª–∏—á–µ—Å–∫–∏–π',
            '–¥–µ—Ä–µ–≤–æ', '–¥–µ—Ä–µ–≤—è–Ω–Ω—ã–π',
            '–∫–æ–≤–∫–∞', '–∫–æ–≤–∞–Ω—ã–π'
        ]
        
        text_lower = text.lower()
        found = []
        for material in materials:
            if material in text_lower:
                found.append(material)
        
        return ' '.join(found)
    
    def _extract_color(self, text: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ü–≤–µ—Ç–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        colors = [
            '–±–µ–ª—ã–π', '—á–µ—Ä–Ω—ã–π', '—Å–µ—Ä—ã–π', '–±–µ–∂–µ–≤—ã–π',
            '–∫–æ—Ä–∏—á–Ω–µ–≤—ã–π', '—Å–∏–Ω–∏–π', '–≥–æ–ª—É–±–æ–π', '–∑–µ–ª–µ–Ω—ã–π',
            '–∫—Ä–∞—Å–Ω—ã–π', '—Ä–æ–∑–æ–≤—ã–π', '–∂–µ–ª—Ç—ã–π', '–æ—Ä–∞–Ω–∂–µ–≤—ã–π',
            '—Ñ–∏–æ–ª–µ—Ç–æ–≤—ã–π', '–∑–æ–ª–æ—Ç–æ–π', '—Å–µ—Ä–µ–±—Ä—è–Ω—ã–π',
            '–±—Ä–æ–Ω–∑–æ–≤—ã–π', '–º–µ–¥–Ω—ã–π'
        ]
        
        text_lower = text.lower()
        found = []
        for color in colors:
            if color in text_lower:
                found.append(color)
        
        return ' '.join(found)
    
    def _extract_type(self, text: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–∏–ø–∞ —Ç–æ–≤–∞—Ä–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        types = {
            '–∫–∞—Ä–Ω–∏–∑—ã': ['–∫–∞—Ä–Ω–∏–∑', '—à—Ç–∞–Ω–≥–∞', '—Ç—Ä—É–±–∞'],
            '—à—Ç–æ—Ä—ã': ['—à—Ç–æ—Ä', '–∑–∞–Ω–∞–≤–µ—Å', '–ø–æ—Ä—Ç—å–µ—Ä'],
            '—Ç—é–ª—å': ['—Ç—é–ª—å', '–≤—É–∞–ª—å', '–æ—Ä–≥–∞–Ω–∑–∞'],
            '—Ä—É–ª–æ–Ω–Ω—ã–µ': ['—Ä—É–ª–æ–Ω–Ω', '—Ä–æ–ª—å—à—Ç–æ—Ä', '—Ä–æ–ª–µ—Ç'],
            '–∂–∞–ª—é–∑–∏': ['–∂–∞–ª—é–∑–∏', '–ª–∞–º–µ–ª'],
            '—Ä–∏–º—Å–∫–∏–µ': ['—Ä–∏–º—Å–∫'],
        }
        
        text_lower = text.lower()
        for type_name, keywords in types.items():
            for keyword in keywords:
                if keyword in text_lower:
                    return type_name
        
        return ""
    
    def train_from_excel_data(self, products: List[Dict]) -> Dict:
        """
        –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ Excel
        
        Args:
            products: –°–ø–∏—Å–æ–∫ —Ç–æ–≤–∞—Ä–æ–≤ —Å –ø–æ–ª—è–º–∏:
                - nm_id, name, category, group_id, price
        
        Returns:
            –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—É—á–µ–Ω–∏—è
        """
        print("üéì –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ Excel –¥–∞–Ω–Ω—ã—Ö...")
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Ç–æ–≤–∞—Ä—ã –ø–æ group_id –∏–∑ Excel
        groups = defaultdict(list)
        for product in products:
            group_id = product.get('group_id') or product.get('ID —Å–∫–ª–µ–π–∫–∏')
            if group_id and group_id != 'nan' and str(group_id).strip():
                groups[str(group_id)].append(product)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –≤—Å–µ—Ö —Ç–æ–≤–∞—Ä–æ–≤
        all_features = []
        all_products = []
        
        for group_id, group_products in groups.items():
            if len(group_products) < 2:  # –ì—Ä—É–ø–ø–∞ –¥–æ–ª–∂–Ω–∞ —Å–æ–¥–µ—Ä–∂–∞—Ç—å –º–∏–Ω–∏–º—É–º 2 —Ç–æ–≤–∞—Ä–∞
                continue
            
            for product in group_products:
                features = self.extract_features(product)
                all_features.append(features)
                all_products.append(product)
        
        # –û–±—É—á–∞–µ–º –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä
        if len(all_features) > 0:
            self.vectorizer.fit(all_features)
            self.trained = True
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        for group_id, group_products in groups.items():
            categories = set(p.get('category', '') for p in group_products if p.get('category'))
            materials = set()
            sizes = set()
            
            for product in group_products:
                name = product.get('name', '')
                materials.add(self._extract_material(name))
                sizes.add(self._extract_size(name))
            
            if categories:
                main_category = list(categories)[0]
                if main_category not in self.category_patterns:
                    self.category_patterns[main_category] = {
                        'materials': set(),
                        'sizes': set(),
                        'groups': []
                    }
                
                self.category_patterns[main_category]['materials'].update(
                    m for m in materials if m
                )
                self.category_patterns[main_category]['sizes'].update(
                    s for s in sizes if s
                )
                self.category_patterns[main_category]['groups'].append(group_id)
        
        stats = {
            'total_products': len(all_products),
            'total_groups': len(groups),
            'categories': len(self.category_patterns),
            'trained': self.trained,
            'avg_group_size': np.mean([len(g) for g in groups.values()]) if groups else 0
        }
        
        print(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        print(f"   - –¢–æ–≤–∞—Ä–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {stats['total_products']}")
        print(f"   - –ì—Ä—É–ø–ø –Ω–∞–π–¥–µ–Ω–æ: {stats['total_groups']}")
        print(f"   - –ö–∞—Ç–µ–≥–æ—Ä–∏–π: {stats['categories']}")
        print(f"   - –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä –≥—Ä—É–ø–ø—ã: {stats['avg_group_size']:.1f}")
        
        return stats
    
    def find_similar_products(self, target_product: Dict, candidate_products: List[Dict], 
                             top_k: int = 20) -> List[Tuple[Dict, float]]:
        """
        –ù–∞—Ö–æ–¥–∏—Ç –ø–æ—Ö–æ–∂–∏–µ —Ç–æ–≤–∞—Ä—ã –¥–ª—è —Ü–µ–ª–µ–≤–æ–≥–æ —Ç–æ–≤–∞—Ä–∞
        
        Args:
            target_product: –¶–µ–ª–µ–≤–æ–π —Ç–æ–≤–∞—Ä
            candidate_products: –°–ø–∏—Å–æ–∫ —Ç–æ–≤–∞—Ä–æ–≤-–∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        
        Returns:
            –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (—Ç–æ–≤–∞—Ä, —Å—Ö–æ–∂–µ—Å—Ç—å)
        """
        if not self.trained:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞! –í—ã–∑–æ–≤–∏—Ç–µ train_from_excel_data() —Å–Ω–∞—á–∞–ª–∞")
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ —Ü–µ–ª–µ–≤–æ–≥–æ —Ç–æ–≤–∞—Ä–∞
        target_features = self.extract_features(target_product)
        target_vector = self.vectorizer.transform([target_features])
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        target_category = target_product.get('category', '')
        filtered_candidates = [
            p for p in candidate_products 
            if p.get('category', '') == target_category
            and p.get('nm_id') != target_product.get('nm_id')
        ]
        
        if not filtered_candidates:
            return []
        
        # –í–µ–∫—Ç–æ—Ä–∏–∑—É–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
        candidate_features = [self.extract_features(p) for p in filtered_candidates]
        candidate_vectors = self.vectorizer.transform(candidate_features)
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å
        similarities = cosine_similarity(target_vector, candidate_vectors)[0]
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞ —Å—Ö–æ–∂–µ—Å—Ç–∏
        adjusted_similarities = []
        for i, (product, sim) in enumerate(zip(filtered_candidates, similarities)):
            adjusted_sim = sim
            
            # –ë–æ–Ω—É—Å –∑–∞ —Å—Ö–æ–∂–∏–π —Ä–∞–∑–º–µ—Ä
            target_size = self._extract_size(target_product.get('name', ''))
            candidate_size = self._extract_size(product.get('name', ''))
            if target_size and candidate_size and target_size == candidate_size:
                adjusted_sim += 0.1
            
            # –ë–æ–Ω—É—Å –∑–∞ —Å—Ö–æ–∂–∏–π –º–∞—Ç–µ—Ä–∏–∞–ª
            target_material = self._extract_material(target_product.get('name', ''))
            candidate_material = self._extract_material(product.get('name', ''))
            if target_material and candidate_material:
                common_materials = set(target_material.split()) & set(candidate_material.split())
                if common_materials:
                    adjusted_sim += 0.15
            
            # –®—Ç—Ä–∞—Ñ –∑–∞ —Å–∏–ª—å–Ω–æ–µ –æ—Ç–ª–∏—á–∏–µ –≤ —Ü–µ–Ω–µ (>2x —Ä–∞–∑–Ω–∏—Ü–∞)
            target_price = target_product.get('price', 0) or target_product.get('current_price', 0)
            candidate_price = product.get('price', 0) or product.get('current_price', 0)
            
            if target_price > 0 and candidate_price > 0:
                price_ratio = max(target_price, candidate_price) / min(target_price, candidate_price)
                if price_ratio > 2.0:
                    adjusted_sim *= 0.7
            
            adjusted_similarities.append(adjusted_sim)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Å—Ö–æ–∂–µ—Å—Ç–∏
        similarities_array = np.array(adjusted_similarities)
        top_indices = np.argsort(similarities_array)[::-1][:top_k]
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –ø–æ—Ä–æ–≥—É —Å—Ö–æ–∂–µ—Å—Ç–∏
        results = []
        for idx in top_indices:
            if similarities_array[idx] >= self.similarity_threshold:
                results.append((filtered_candidates[idx], float(similarities_array[idx])))
        
        return results
    
    def auto_group_new_product(self, new_product: Dict, existing_products: List[Dict]) -> Dict:
        """
        –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –≥—Ä—É–ø–ø—É –¥–ª—è –Ω–æ–≤–æ–≥–æ —Ç–æ–≤–∞—Ä–∞
        
        Args:
            new_product: –ù–æ–≤—ã–π —Ç–æ–≤–∞—Ä –∏–∑ WB API
            existing_products: –°—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ç–æ–≤–∞—Ä—ã –≤ –±–∞–∑–µ
        
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ —Å –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞–º–∏
        """
        similar = self.find_similar_products(new_product, existing_products, top_k=20)
        
        return {
            'product': new_product,
            'competitors': [
                {
                    'nm_id': p.get('nm_id'),
                    'name': p.get('name'),
                    'price': p.get('price') or p.get('current_price'),
                    'category': p.get('category'),
                    'similarity': sim,
                    'confidence': 'high' if sim >= 0.85 else 'medium' if sim >= 0.75 else 'low'
                }
                for p, sim in similar
            ],
            'total_competitors': len(similar),
            'avg_similarity': np.mean([sim for _, sim in similar]) if similar else 0.0
        }
    
    def save_model(self, filepath: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        model_data = {
            'vectorizer': self.vectorizer,
            'category_patterns': {
                k: {
                    'materials': list(v['materials']),
                    'sizes': list(v['sizes']),
                    'groups': v['groups']
                }
                for k, v in self.category_patterns.items()
            },
            'trained': self.trained,
            'similarity_threshold': self.similarity_threshold
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
        
        print(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {filepath}")
    
    def load_model(self, filepath: str):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        with open(filepath, 'rb') as f:
            model_data = pickle.load(f)
        
        self.vectorizer = model_data['vectorizer']
        self.category_patterns = {
            k: {
                'materials': set(v['materials']),
                'sizes': set(v['sizes']),
                'groups': v['groups']
            }
            for k, v in model_data['category_patterns'].items()
        }
        self.trained = model_data['trained']
        self.similarity_threshold = model_data.get('similarity_threshold', 0.75)
        
        print(f"üìÇ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {filepath}")


def demo_training():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏"""
    # –ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö –∏–∑ Excel
    excel_products = [
        {'nm_id': '123', 'name': '–®—Ç–æ—Ä—ã –±–ª—ç–∫–∞—É—Ç 2 —à—Ç 150—Ö250 —Å–º', 'category': '–ü–æ—Ä—Ç—å–µ—Ä—ã', 'group_id': 'G001', 'price': 1500},
        {'nm_id': '124', 'name': '–ü–æ—Ä—Ç—å–µ—Ä—ã –±–ª—ç–∫–∞—É—Ç 150x250', 'category': '–ü–æ—Ä—Ç—å–µ—Ä—ã', 'group_id': 'G001', 'price': 1600},
        {'nm_id': '125', 'name': '–ó–∞–Ω–∞–≤–µ—Å–∫–∏ blackout 150*250 —Å–º', 'category': '–ü–æ—Ä—Ç—å–µ—Ä—ã', 'group_id': 'G001', 'price': 1450},
        
        {'nm_id': '201', 'name': '–®—Ç–æ—Ä—ã –æ–¥–Ω–æ—Ç–æ–Ω 2 —à—Ç 150—Ö250 —Å–º', 'category': '–ü–æ—Ä—Ç—å–µ—Ä—ã', 'group_id': 'G002', 'price': 850},
        {'nm_id': '202', 'name': '–ü–æ—Ä—Ç—å–µ—Ä—ã –æ–¥–Ω–æ—Ç–æ–Ω–Ω—ã–µ 150x250', 'category': '–ü–æ—Ä—Ç—å–µ—Ä—ã', 'group_id': 'G002', 'price': 900},
        
        {'nm_id': '301', 'name': '–ö–∞—Ä–Ω–∏–∑ –∞–ª—é–º–∏–Ω–∏–µ–≤—ã–π 200 —Å–º', 'category': '–ö–∞—Ä–Ω–∏–∑—ã', 'group_id': 'G003', 'price': 1200},
        {'nm_id': '302', 'name': '–®—Ç–∞–Ω–≥–∞ –∞–ª—é–º–∏–Ω–∏–π 2 –º–µ—Ç—Ä–∞', 'category': '–ö–∞—Ä–Ω–∏–∑—ã', 'group_id': 'G003', 'price': 1150},
    ]
    
    # –û–±—É—á–µ–Ω–∏–µ
    engine = MLGroupingEngine()
    stats = engine.train_from_excel_data(excel_products)
    
    # –¢–µ—Å—Ç: –Ω–æ–≤—ã–π —Ç–æ–≤–∞—Ä
    new_product = {
        'nm_id': '999',
        'name': '–®—Ç–æ—Ä—ã –±–ª—ç–∫–∞—É—Ç 150—Ö250 —Å–º –∫–æ–º–ø–ª–µ–∫—Ç',
        'category': '–ü–æ—Ä—Ç—å–µ—Ä—ã',
        'price': 1550
    }
    
    result = engine.auto_group_new_product(new_product, excel_products)
    
    print("\n" + "="*60)
    print(f"üÜï –ù–æ–≤—ã–π —Ç–æ–≤–∞—Ä: {new_product['name']}")
    print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤: {result['total_competitors']}")
    print(f"üìä –°—Ä–µ–¥–Ω—è—è —Å—Ö–æ–∂–µ—Å—Ç—å: {result['avg_similarity']:.2%}")
    print("\n–¢–û–ü –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã:")
    for comp in result['competitors'][:5]:
        print(f"  - {comp['name']} | –°—Ö–æ–∂–µ—Å—Ç—å: {comp['similarity']:.2%} | {comp['confidence'].upper()}")
    
    return engine


if __name__ == '__main__':
    demo_training()
